import os, random, time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


SEED = 42
os.environ["PYTHONHASHSEED"] = str(SEED)
random.seed(SEED)
np.random.seed(SEED)


from statsmodels.datasets import get_rdataset
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.inspection import permutation_importance
from sklearn.base import BaseEstimator

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam

import shap


data = get_rdataset("AirPassengers").data
df = pd.DataFrame({
    "Month": pd.date_range(start="1949-01-01", periods=len(data["value"]), freq="MS"),
    "Passengers": data["value"].astype(float)
})
df.set_index("Month", inplace=True)

df["lag1"] = df["Passengers"].shift(1)
df["lag12"] = df["Passengers"].shift(12)
df["month"] = df.index.month
df["trend"] = np.arange(len(df))
df = df.dropna().copy()


n = len(df)
train_end = int(n * 0.7)
valid_end = train_end + int(n * 0.15)

train = df.iloc[:train_end]
valid = df.iloc[train_end:valid_end]
test = df.iloc[valid_end:]

print("Sizes -> train, valid, test:", len(train), len(valid), len(test))


scaler = MinMaxScaler()
train_scaled = scaler.fit_transform(train)
valid_scaled = scaler.transform(valid)
test_scaled = scaler.transform(test)

def create_sequences(arr, window=12):
    X, y = [], []
    for i in range(window, len(arr)):
        X.append(arr[i-window:i])
        y.append(arr[i, 0])  # passengers is column 0
    return np.array(X), np.array(y)

WINDOW = 12
X_train, y_train = create_sequences(train_scaled, WINDOW)
X_valid, y_valid = create_sequences(valid_scaled, WINDOW)
X_test, y_test = create_sequences(test_scaled, WINDOW)

print("Sequence shapes:", X_train.shape, X_valid.shape, X_test.shape)


tf.keras.backend.clear_session()
model = Sequential([
    LSTM(32, return_sequences=False, input_shape=(WINDOW, train_scaled.shape[1])),
    Dropout(0.2),
    Dense(1)
])
model.compile(optimizer=Adam(learning_rate=0.005), loss="mse", metrics=["mae"])

es = EarlyStopping(monitor="val_loss", patience=8, restore_best_weights=True, verbose=0)
t0 = time.time()
model.fit(X_train, y_train, validation_data=(X_valid, y_valid),
          epochs=100, batch_size=8, callbacks=[es], verbose=0)
print("Training time: {:.1f}s".format(time.time() - t0))


pred_test_scaled = model.predict(X_test, verbose=0).reshape(-1,1)
pad = np.zeros((len(pred_test_scaled), train_scaled.shape[1]-1))
pred_test_full = np.hstack((pred_test_scaled, pad))
pred_test = scaler.inverse_transform(pred_test_full)[:, 0]

y_test_full = np.hstack((y_test.reshape(-1,1), np.zeros((len(y_test), train_scaled.shape[1]-1))))
y_test_actual = scaler.inverse_transform(y_test_full)[:, 0]

rmse = np.sqrt(mean_squared_error(y_test_actual, pred_test))
mae = mean_absolute_error(y_test_actual, pred_test)
print(f"Test RMSE: {rmse:.3f}, MAE: {mae:.3f}")


plt.figure(figsize=(9,4))
test_dates_for_pred = test.index[WINDOW:]
plt.plot(df.index, df["Passengers"], label="All (actual)", linewidth=1)
plt.plot(test_dates_for_pred, pred_test, label="LSTM pred (test)", linestyle="--")
plt.legend(); plt.title("Simple LSTM Predictions"); plt.grid(alpha=0.3); plt.tight_layout()
plt.show()


class LSTMWrapper(BaseEstimator):
    def __init__(self, model, scaler, n_features, window):
        self.model = model
        self.scaler = scaler
        self.n_features = n_features
        self.window = window
    def fit(self, X, y):
        return self
    def predict(self, X_flat):
        X3 = X_flat.reshape(-1, self.window, self.n_features)
        p = self.model.predict(X3, verbose=0)
        full = np.hstack((p.reshape(-1,1), np.zeros((len(p), self.n_features-1))))
        return self.scaler.inverse_transform(full)[:, 0]

X_valid_flat = X_valid.reshape(X_valid.shape[0], -1)
y_valid_actual = scaler.inverse_transform(
    np.hstack((y_valid.reshape(-1,1), np.zeros((len(y_valid), train_scaled.shape[1]-1))))
)[:, 0]

wrapper = LSTMWrapper(model, scaler, train_scaled.shape[1], WINDOW)
print("Computing permutation importance (may take ~10-30s)...")
perm = permutation_importance(wrapper, X_valid_flat, y_valid_actual,
                              scoring="neg_mean_squared_error", n_repeats=5, random_state=SEED, n_jobs=1)

feat_names = list(train.columns)
n_feat = len(feat_names)
flat_len = perm.importances_mean.shape[0]
importance_per_feature = []
for f in range(n_feat):
    idxs = list(range(f, flat_len, n_feat))
    importance_per_feature.append(perm.importances_mean[idxs].mean())

imp_df = pd.DataFrame({"feature": feat_names, "importance": importance_per_feature}).sort_values("importance", ascending=False)
print("Permutation importance (top):")
print(imp_df.head(6))


print("Computing SHAP values (small sample)...")
background_size = min(30, X_train.shape[0])
background = X_train[np.random.choice(X_train.shape[0], background_size, replace=False)]

shap_values = None
try:
    expl = shap.GradientExplainer(model, background)
    shap_values = expl.shap_values(X_test[:30])
    print("Used GradientExplainer")
except Exception as e:
    print("GradientExplainer failed, falling back to KernelExplainer:", e)
    def predict_flat(x_flat):
        x3 = x_flat.reshape(-1, WINDOW, train_scaled.shape[1])
        out = model.predict(x3, verbose=0).reshape(-1)
        return out
    background_flat = background.reshape(background.shape[0], -1)
    X_test_flat = X_test[:30].reshape(30, -1)
    expl = shap.KernelExplainer(predict_flat, background_flat)
    shap_vals_flat = expl.shap_values(X_test_flat, nsamples=100)
    # kernel returns list or array; normalize to list with one array shaped (samples, time, features)
    shap_values = [np.array(shap_vals_flat).reshape(30, WINDOW, train_scaled.shape[1])]

arr = None
if isinstance(shap_values, list):
    # pick the first element as the array
    arr = np.array(shap_values[0])
else:
    arr = np.array(shap_values)

print("raw shap array shape:", arr.shape)


if arr.ndim == 2:
    # try to infer feature count; reshape to (samples, window, features)
    arr = arr.reshape(arr.shape[0], WINDOW, -1)
    print("reshaped shap to (samples, time, features):", arr.shape)


if arr.ndim == 3 and arr.shape[2] != len(feat_names) and arr.shape[1] == len(feat_names):
    arr = np.transpose(arr, (0,2,1))
    print("transposed shap to (samples, time, features):", arr.shape)


if not (arr.ndim == 3 and arr.shape[2] == len(feat_names)):
  
    try:
        arr = arr.reshape(arr.shape[0], WINDOW, len(feat_names))
        print("fallback reshaped shap to:", arr.shape)
    except Exception as e:
        raise ValueError(f"Cannot interpret SHAP output shape {arr.shape} automatically.") from e


shap_abs = np.mean(np.abs(arr), axis=(0,1))  # result shape (n_features,)
print("aggregated shap abs shape:", shap_abs.shape)

shap_df = pd.DataFrame({"feature": feat_names, "shap_importance": shap_abs}).sort_values("shap_importance", ascending=False)
print("SHAP importance (top):")
print(shap_df.head(6))


common = set(imp_df.head(5)['feature']).intersection(set(shap_df.head(5)['feature']))
print("\nCommon top features between Permutation and SHAP:", common)
print("Done.")
